{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52f7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2529f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams = pd.read_csv('C:\\\\Users\\\\bookw\\\\Dropbox\\\\Capstone\\\\DreamBERT\\\\00_data\\\\dream_text_corpus')\n",
    "dream = dreams['dreams']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d60d8d",
   "metadata": {},
   "source": [
    "## First attempt from:\n",
    "https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store values in a dataframe column (Series object) to files, one file per record\n",
    "def column_to_files(column, prefix, txt_files_dir):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=prefix\n",
    "    #For every value in the df, with just one column\n",
    "    for row in column.to_list():\n",
    "      # Create the filename using the prefix ID\n",
    "      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
    "      try:\n",
    "        # Create the file and write the column text to it\n",
    "        f = open(file_name, 'wb')\n",
    "        f.write(row.encode('utf-8'))\n",
    "        f.close()\n",
    "      except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "        print(row, e) \n",
    "      i+=1\n",
    "    # Return the last ID\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/bookw/Dropbox/Capstone/data_files'\n",
    "# Get the training data\n",
    "data = dreams[\"dreams\"]\n",
    "# Removing the end of line character \\n\n",
    "data = data.replace(\"\\n\",\" \")\n",
    "# Set the ID to 0\n",
    "prefix=0\n",
    "print(len(data))\n",
    "# Create a file for every description value\n",
    "prefix = column_to_files(data, prefix, path)\n",
    "# Print the last ID\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521dd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])\n",
    "#Save the Tokenizer to disk\n",
    "tokenizer.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(path,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(path,'merges.txt'))\n",
    ")\n",
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# Test the tokenizer\n",
    "tokenizer.encode(\"knit midi dress with vneckline straps.\")\n",
    "# Show the tokens created\n",
    "tokenizer.encode(\"knit midi dress with vneckline straps.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a configuration for our RoBERTa model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=8192,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df56f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer from a trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(path, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2299d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dreams['dreams'][0:25000]\n",
    "eval_dataset = dreams['dreams'][25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e92165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=path,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=.0001,\n",
    "    weight_decay=.01,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    save_steps=8192,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Create a Fill mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=path,\n",
    "    tokenizer=path\n",
    ")\n",
    "# Test some examples\n",
    "# knit midi dress with vneckline\n",
    "# =>\n",
    "fill_mask(\"midi <mask> with vneckline.\")\n",
    "# The test text: Round neck sweater with long sleeves\n",
    "fill_mask(\"Round neck sweater with <mask> sleeves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad197e7d",
   "metadata": {},
   "source": [
    "## Second attempt from:\n",
    "https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(\"./data_files/\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\".\", \"DREAMbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b362b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec33f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"DREAMbert-vocab.json\",\n",
    "    \"DREAMbert-merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"I had a dream.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DREAMbertDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \"DREAMbert-vocab.json\",\n",
    "            \"DREAMbert-merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./data_files/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data_files/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "--output_dir './models/DREAMbert'\n",
    "--model_type bert\n",
    "--mlm\n",
    "--do_train\n",
    "--do_eval\n",
    "--learning_rate 1e-4\n",
    "--num_train_epochs 5\n",
    "--save_total_limit 2\n",
    "--save_steps 2000\n",
    "--per_gpu_train_batch_size 16\n",
    "--evaluate_during_training\n",
    "--seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91167c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./models/DREAMbert\",\n",
    "    tokenizer=\"./models/DREAMbert\"\n",
    ")\n",
    "\n",
    "result = fill_mask(\"I had a <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff98e8",
   "metadata": {},
   "source": [
    "## Third attempt?\n",
    "https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3813817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "#trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "#tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filess = []\n",
    "\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\bookw\\\\Dropbox\\\\Capstone\\\\DreamBERT\\\\20_preliminary_modeling\\\\data_files'):\n",
    "    for file in files:\n",
    "        filess.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548151bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(filess, vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./dreambert')\n",
    "tokenizer.save_model('dreambert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb71851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig, BertTokenizer, BertForMaskedLM, AutoTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dreambert', max_len=512)\n",
    "#tokenizer = ByteLevelBPETokenizer(\n",
    "#    \"dreambert/vocab.json\",\n",
    "#    \"dreambert/merges.txt\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da697e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_files/0.txt', 'r', encoding='utf-8') as fp:\n",
    "    lines = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6fcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(lines, max_length=512, padding='max_length', truncation=True)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([x.ids for x in batch])\n",
    "mask = torch.tensor([x.attention_mask for x in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of labels tensor, this will be input_ids\n",
    "input_ids = labels.detach().clone()\n",
    "# create random array of floats with equal dims to input_ids\n",
    "rand = torch.rand(input_ids.shape)\n",
    "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
    "# loop through each row in input_ids tensor (cannot do in parallel)\n",
    "for i in range(input_ids.shape[0]):\n",
    "    # get indices of mask positions from mask array\n",
    "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    # mask input_ids\n",
    "    input_ids[i, selection] = 3  # our custom [MASK] token == 3\n",
    "    \n",
    "    \n",
    "\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7313e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833837c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28798998",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "result = fill_mask(\"I had a <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f265ce",
   "metadata": {},
   "source": [
    "## Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DREAMbert_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "DREAMbert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "DREAMbert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "DREAMbert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "DREAMbert_tokenizer.train(filess, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DREAMbert_tokenizer.save(\"dreambert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac93a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DREAMbert_tokenizer=Tokenizer.from_file(\"dreambert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fc635",
   "metadata": {},
   "source": [
    "## Five: From Pretrained\n",
    "https://huggingface.co/transformers/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ec7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel, BertConfig, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387a46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(filess, padding=\"max_length\", truncation=True)\n",
    "small_train_dataset = dreams['dreams'][0:100]\n",
    "small_eval_dataset = dreams['dreams'][100:200]\n",
    "train_dataset = dreams['dreams'][0:25000]\n",
    "eval_dataset = dreams['dreams'][25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7dbfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tok_small_train_dataset = []\n",
    "for i in small_train_dataset:\n",
    "    tok_small_train_dataset.append(tokenize_function(i))\n",
    "    \n",
    "tok_small_eval_dataset = []\n",
    "for i in small_eval_dataset:\n",
    "    tok_small_eval_dataset.append(tokenize_function(i))\n",
    "    \n",
    "tok_train_dataset = []\n",
    "for i in train_dataset:\n",
    "    tok_train_dataset.append(tokenize_function(i))\n",
    "    \n",
    "tok_eval_dataset = []\n",
    "for i in eval_dataset:\n",
    "    tok_eval_dataset.append(tokenize_function(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ec4f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b1a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", per_device_train_batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdafb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=tok_small_train_dataset, eval_dataset=tok_small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d36772",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0fb61d6f4560>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#this happens here, on Google colab, and in ACCRE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#am I actually utilizing my GPUs?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#try it with a sample dataset from pytorch?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1262\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1264\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1736\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1738\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m             \u001b[1;31m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1780\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m   1770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1772\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1773\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "#I get an error that just says \"loss\" (do I need to specify a loss function since there are no labels?) and then when I run it again I get a \"CUDA out of memory\" error\n",
    "#this happens here, on Google colab, and in ACCRE\n",
    "#am I actually utilizing my GPUs?\n",
    "trainer.train()\n",
    "#try it with a sample dataset from pytorch?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
